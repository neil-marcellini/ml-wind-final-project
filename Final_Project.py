# -*- coding: utf-8 -*-
"""Project_test.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ORU0R8NVz-pMJM9xbId-Jtnv9Z8mSR5-

First I downloaded the 2013 Americas Cup data from the [Sailing Yacht Research Foundation's archive](https://www.sailyachtresearch.org/resources/34th-americas-cup-liveline/). I also found data from the 2017 America's Cup World Series [here](http://noticeboard.acracemgt.com/home/race-data).

The data is highly structured according to their [data export format specification](https://www.sailyachtresearch.org/wp-content/uploads/2017/03/AC_AC34-Race-Data-Export-Format-Specification.pdf). Below is some code I wrote to unarchive all of the .zip files, and then extract all the of the sailing data. Many of the files included initially contained gps data of marks and race committee boats. The code is included here to show what I have done, but it doesn't need to be run, since I have uploaded all of the csv files to google drive.
"""

from zipfile import ZipFile
import xml.etree.ElementTree as et
import os
import shutil


def unpack_zip(zipfile='', path_from_local=''):
    filepath = path_from_local+zipfile
    extract_path = filepath.strip('.zip')+'/'
    parent_archive = ZipFile(filepath)
    parent_archive.extractall(extract_path)
    namelist = parent_archive.namelist()
    parent_archive.close()
    for name in namelist:
        try:
            if name[-4:] == '.zip':
                unpack_zip(zipfile=name, path_from_local=extract_path)
        except:
            print('failed on', name)
            pass
    return extract_path

# you can just call this with filename set to the relative path and file.


# used to unpack all zips in AC34 folder since unpack_zip didn't work
def unpack_ac34():
    contents = os.listdir("sailing_data/AC34/")
    for zip_file in contents:
        if zip_file.endswith(".zip"):
            unpack_zip("sailing_data/AC34/" + zip_file)

# unpack_ac34()


# get boats from xml where Type=Yacht
# make a list of all StoweName entries
def get_boats(xml_root):
    boat_names = []
    boats = xml_root.find("Boats")
    if boats is None:
        print("Error, no boats")
        return boat_names
    yachts = [boat for boat in boats if boat.get("Type") == "Yacht"]
    boat_names = [boat.get("StoweName") for boat in yachts]
    return boat_names

# get all NAV csvs matching StoweName
def get_csv_files(root, race_boats):
    csv_root = os.path.join(root, "csv")
    csv_files = os.listdir(csv_root)
    csv_files = [os.path.join(csv_root, csv) for csv in csv_files if csv.endswith(".csv")]
    nav_csvs = [csv for csv in csv_files if "NAV" in csv]
    boat_csvs = []
    for nav in nav_csvs:
        for stowe_name in race_boats:
            if stowe_name in nav:
                boat_csvs.append(nav)
    return boat_csvs



# extract all csv files from sailboats
def extract_yacht_csvs():
    # walk through all files and folders
    for root, dirs, files in os.walk("sailing_data"):
        # open the history folder and boats.xml
        if "history" in dirs and "csv" in dirs:
            xml_files = os.listdir(os.path.join(root,"history"))
            boat_files = [xml for xml in xml_files if "_boats" in xml]
            if len(boat_files) < 1:
                print(f"Error, {len(boat_files)} boat files")
                continue
            # get unique boats in this race from all boats.xml files
            race_boats = set()
            for boat_file in boat_files:
                tree = et.parse(os.path.join(root, "history", boat_file))
                xml_root = tree.getroot()
                boats = get_boats(xml_root)
                race_boats.update(boats)
            print(f"raceboats = {race_boats}")
            boat_csvs = get_csv_files(root, race_boats)
            print("boat_csvs = ", boat_csvs)
            # move each csv file into the tracks folder
            for boat_csv in boat_csvs:
                shutil.copy(boat_csv, "sailing_data/ac_tracks")


extract_yacht_csvs()

"""Here we are mounting my google drive to include the uploaded sailing data."""

# load all files from google drive
from google.colab import drive
# drive.flush_and_unmount()
drive.mount('/content/drive', force_remount=True)

"""Here is all of our code for pre-processing the data."""

'''
Adam Bisagne
Neil Marcellini
Final Project Progress Report
'''


import pandas as pd
import numpy as np
from sklearn.linear_model import LinearRegression
from sklearn import model_selection, metrics
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from google.colab import drive
import os
from datetime import datetime
import statistics
import math
import matplotlib.pyplot as plt



class HyperParams:
  # object to hold all hyper parameters
  features = ["timestamp", "Lat", "Lon", "COG","SOG", "COG_delta", "SOG_delta"]
  # the number of past minutes to look at
  minutes = 3
  size = 60
  target = "CourseWindDirection"
  # number of csv files to use from the dataset
  num_csvs = 6
  base_path = "/content/drive/MyDrive/sailing_dataset/"
  best_lr = None
  best_activation = "relu"

hypes = HyperParams()

def segment_data(data_set, hypes=hypes):
    size = hypes.size * hypes.minutes
    segments = []
    for i in range(size, len(data_set)):
        sample = np.array(data_set[i-size: i])
        sample = sample.flatten()
        segments.append(sample)
    return segments


def samples_per_second(data):
  # return the number of samples per second
  samples_per_s = []
  seconds_to_check = 5
  i = 0
  timestamp = data["timestamp"][0]
  prev_second = datetime.fromtimestamp(timestamp).second
  count_samples = 0
  while len(samples_per_s) < seconds_to_check and i < len(data):
    timestamp = data["timestamp"][i]
    curr_second = datetime.fromtimestamp(timestamp).second
    if curr_second == prev_second:
      count_samples += 1
    else:
      samples_per_s.append(count_samples)
      count_samples = 0
    prev_second = curr_second
    i += 1
  if len(samples_per_s) == 0:
    return 0
  else:
    return statistics.mode(samples_per_s)


def group_data(data, hypes=hypes):
  size = hypes.size
  minutes = hypes.minutes
  list_seconds = []
  list_wind = []
  for i in data.index:
      list_seconds.append(data.loc[i, hypes.features])
      list_wind.append(data.loc[i, hypes.target])
 
  data_return = pd.DataFrame(list_seconds, columns = hypes.features)

  data_labels = pd.DataFrame(list_wind[(size*minutes):], columns=[hypes.target])

  return segment_data(data_set = data_return), data_labels


  

def once_per_second(data):   
  # transform data to have 1 sample per second
  split = samples_per_second(data)
  if split == 0:
    return data, False
  else:
    indexes = [*range(0, len(data), split)]
    new_data = data.iloc[indexes]
    return new_data, True



def valid_date_field(csv_df):
  # not all csv files have a valid date field
  first_date = csv_df["Date"][0]
  # try to convert to a date
  date_format = "%d:%m:%Y"
  try:
    datetime.strptime(first_date, date_format)
  except ValueError as v_error:
    # not a valid date
    return False
  return True

def date_from_name(csv_df, csv_file):
  # use the date encoded in the file name
  # set the date for all rows
  date_format = "%Y%m%d"
  # get first 8 characters of file name
  date_string = csv_file[:8]
  file_date = datetime.strptime(date_string, date_format)
  new_date = file_date.strftime("%d:%m:%Y")
  num_rows = len(csv_df)
  new_dates = [new_date] * num_rows
  csv_df["Date"] = new_dates
  return csv_df

def add_timestamps(df):
  # given the dataframe, add a column with unix timestamps

  # assume date is the same for the whole file
  first_date = df["Date"][0]
  timestamps = []
  for i in range(len(df)):
    row = df.loc[i]
    # make unix timestamp from date, localtime, and timezone
    localTime = row["LocalTime"]
    timeZone = row["Zone"]
    if timeZone < 0:
      # extract - sign
      offsetSign = str(timeZone)[0]
      utcOffset = str(timeZone)[1:]
    else:
      offsetSign = "+"
      utcOffset = str(timeZone)
    utcOffset += "00"
    # pad with zeros
    utcOffset = utcOffset.zfill(4)
    # prepend sign
    utcOffset = offsetSign + utcOffset
    format_string = "%d:%m:%Y %H:%M:%S.%f %z"
    time_string = " ".join([first_date, localTime, utcOffset])
    sample_datetime = datetime.strptime(time_string, format_string)
    timestamps.append(datetime.timestamp(sample_datetime))
  df["timestamp"] = timestamps
  return df

def add_deltas(df):
  # add change in COG and SOG
  cogs = df["COG"]
  sogs = df["SOG"]
  cog_deltas = np.diff(cogs)
  sog_deltas = np.diff(sogs)
  # first diff is 0, prepend to deltas
  cog_deltas = np.concatenate([[0], cog_deltas])
  sog_deltas = np.concatenate([[0], sog_deltas])
  df["COG_delta"] = cog_deltas
  df["SOG_delta"] = sog_deltas
  return df

def balance_train_data(x_train, y_train, hypes = hypes):
  '''
  Up-sample less frequent wind directions.
  '''
  print("Old shapes")
  print(x_train.shape)
  print(y_train.shape)
  print("")
  wind_dir_freqs = y_train.value_counts(sort=True, ascending=True)
  # boost all less than max
  add_counts = []
  max_freq = wind_dir_freqs.iloc[-1]
  for i in range(len(wind_dir_freqs) - 1):
    wind_dir = wind_dir_freqs.index[i]
    wind_dir_freq = wind_dir_freqs.iloc[i]
    rows_to_add = max_freq - wind_dir_freq
    add_counts.append(rows_to_add)
    
    rel_index = y_train[y_train[hypes.target] == wind_dir].index.values[0]

    row_x = x_train.iloc[rel_index]
    row_y = y_train.iloc[rel_index]

    row_x = pd.DataFrame(row_x).transpose()
    row_y = pd.DataFrame(row_y)

    new_rows_x = pd.concat([row_x]*rows_to_add)
    new_rows_y = pd.concat([row_y]*rows_to_add)

    new_df_x = pd.DataFrame(new_rows_x, columns = x_train.columns)
    new_df_y = pd.DataFrame(new_rows_y, columns = [hypes.target])

    x_train = pd.concat([x_train,new_df_x], ignore_index=True)
    y_train = pd.concat([y_train,new_df_y], ignore_index=True)
    # x_train = x_train.append(x_train, ignore_index=True)
    # y_train = y_train.append(y_train, ignore_index = True)
  
  print("New shapes")
  print(x_train.shape)
  print(y_train.shape)
  after_freqs = y_train.value_counts(sort=True, ascending=True)
  print(f"before freqs equals after_freqs is {wind_dir_freqs.equals(after_freqs)}")

  print("row counts added")
  print(add_counts)

  return x_train,y_train


def frame_from_csvs(csv_files, base_path, hypes=hypes):
  # make huge dataframe from csv_files
  dfx = pd.DataFrame()
  dfy = pd.DataFrame(columns=[hypes.target])
  for csv_file in csv_files:
    csv_df = pd.read_csv(os.path.join(base_path, csv_file))
    if not valid_date_field(csv_df):
      csv_df = date_from_name(csv_df, csv_file)
    csv_df = add_timestamps(csv_df)
    csv_df, isValid = once_per_second(csv_df)
    if isValid:
      csv_df = add_deltas(csv_df)
      x_train, y_train = group_data(csv_df)
      dfx = dfx.append(x_train, ignore_index=True)
      dfy = dfy.append(y_train, ignore_index = True)
  return dfx, dfy

def save_df(dfx, dfy, type_name, hypes=hypes):
  '''
  Save the pre-processed dataframe
  to a file. Named (train/test/val)_(num_csvs) used.
  '''

  file_name_x = f"{type_name}_x_{hypes.num_csvs}.csv"
  file_path_x = os.path.join(hypes.base_path, "ac_processed/", file_name_x)
  print(f"Saving {file_name_x}")
  # don't save the index in the csv
  dfx.to_csv(file_path_x, index = False)


  file_name_y = f"{type_name}_y_{hypes.num_csvs}.csv"
  file_path_y = os.path.join(hypes.base_path, "ac_processed/", file_name_y)
  print(f"Saving {file_name_y}")
  dfy.to_csv(file_path_y, index = False)


def process_files(validation = False, hypes=hypes):
  print("Processing files")
  num_files = hypes.num_csvs
  tracks_path = os.path.join(hypes.base_path, "ac_tracks/")
  csv_files = os.listdir(tracks_path)
  # only grab the specified number of files
  csv_files = csv_files[:num_files]
  # split off random %20 of the tracks for testing
  train_csvs, test_csvs = model_selection.train_test_split(csv_files, test_size=0.2, random_state=123)
  # split off validation tracks if necessarcy
  if validation:
    train_csvs, val_csvs = model_selection.train_test_split(train_csvs, test_size=0.2, random_state=123)
    print("val_csvs")
    print(val_csvs)
    val_dfx, val_dfy = frame_from_csvs(val_csvs, tracks_path)
    save_df(val_dfx, val_dfy, type_name="val")
  else:
    val_dfx, val_dfy = None, None

  print("train_csvs")
  print(train_csvs)
  print("test_csvs")
  print(test_csvs)

  train_dfx, train_dfy = frame_from_csvs(train_csvs, tracks_path)
  test_dfx, test_dfy = frame_from_csvs(test_csvs, tracks_path)
  
  save_df(train_dfx,train_dfy, type_name="train")
  save_df(test_dfx, test_dfy, type_name="test")
  if validation:
    return train_dfx, train_dfy, val_dfx, val_dfy, test_dfx, test_dfy
  else:
    return train_dfx, train_dfy, test_dfx, test_dfy
  

def load_dataset(hypes=hypes, process = False, validation=False):
  '''
  Load a dataset from the specified number of files.
  Check for preprocessed file mathching num_files first, unless process = True
  '''
  num_files = hypes.num_csvs
  base_path = hypes.base_path
  processed_path = os.path.join(base_path, "ac_processed/")
  print(f"Using {num_files} sailing tracks.")
  
  if process:
    if validation:
      x_train, y_train, x_val, y_val, x_test, y_test = process_files(validation=validation)  
    else:
      x_train, y_train, x_test, y_test = process_files(validation=validation)
  else:
    # check if processed before
    processed_files = os.listdir(processed_path)
    train_name = f"train_x_{hypes.num_csvs}.csv"
    test_name = f"test_x_{hypes.num_csvs}.csv"
    train_name_y = f"train_y_{hypes.num_csvs}.csv"
    test_name_y = f"test_y_{hypes.num_csvs}.csv"
    list_names = [train_name, test_name, train_name_y, test_name_y]
    if validation:
      val_name_x = f"val_x_{hypes.num_csvs}.csv"
      val_name_y = f"val_y_{hypes.num_csvs}.csv"
      list_names.extend([val_name_x, val_name_y])

    already_processed = all([name in processed_files for name in list_names])
    if already_processed:
      print("Loading from ac_processed")
      # load from csvs
      train_index = processed_files.index(train_name)
      test_index = processed_files.index(test_name)

      train_index_y = processed_files.index(train_name_y)
      test_index_y = processed_files.index(test_name_y)
      if validation:
        val_index_x = processed_files.index(val_name_x)
        val_index_y = processed_files.index(val_name_y)
        val_file_x = processed_files[val_index_x]
        val_file_y = processed_files[val_index_y]
        x_val = pd.read_csv(os.path.join(processed_path, val_file_x))
        y_val = pd.read_csv(os.path.join(processed_path, val_file_y))

      train_file_x = processed_files[train_index]
      test_file_x = processed_files[test_index]

      train_file_y = processed_files[train_index_y]
      test_file_y = processed_files[test_index_y]


      x_train = pd.read_csv(os.path.join(processed_path, train_file_x))
      y_train = pd.read_csv(os.path.join(processed_path,train_file_y))
      x_test = pd.read_csv(os.path.join(processed_path, test_file_x))
      y_test = pd.read_csv(os.path.join(processed_path,test_file_y))

      # print(f"x_train cols = {x_train.columns} \n y_train cols = {y_train.columns}\n")
      # print(f"x_test cols = {x_test.columns} \n test cols = {y_test.columns}")

    else:
      # not already processed
      if validation:
        x_train, y_train, x_val, y_val, x_test, y_test = process_files(validation=validation)  
      else:
        x_train, y_train, x_test, y_test = process_files(validation=validation)
  
  # return resulting data
  if validation:
    return x_train, y_train, x_val, y_val, x_test, y_test
  else:
    return x_train, y_train, x_test, y_test

def display_metrics(predictions, y_test, hypes=hypes):
  # make sure we are grabbing target
  y_true = y_test[hypes.target]
  print(f"predictions shape = {predictions.shape}")
  print(f"y_true shape = {y_true.shape}")

  #R2_score
  r2Score = r2_score(y_true, predictions)
  print("R Squared (r2)")
  print(r2Score)

  print("Mean Squared Error (mse)")
  mse = mean_squared_error(y_true, predictions)
  print(mse)

  print("Root Mean Squared Error (rmse)")
  rmse = math.sqrt(mse)
  print(rmse)

  print("Mean Absolute Error (mae)")
  mae = mean_absolute_error(y_true, predictions)
  print(mae)



# process a bunch of data
x_train, y_train, x_test, y_test = load_dataset(process=True)

"""Run a linear regression on the processed data."""

def LinearReg(x_train, y_train, x_test, y_test, normalize = False, hypes=hypes, list_compass_loss=list_compass_loss):
  reg = LinearRegression(normalize = normalize).fit(x_train, y_train)
  predictions = reg.predict(x_test)
  # display_metrics(predictions, y_test)
  loss = list_compass_loss(predictions, y_test.to_numpy())
  print(f"mean compass loss = {loss}")
  return reg


x_train, y_train, x_test, y_test = load_dataset()
reg_model = LinearReg(x_train=x_train, y_train=y_train, x_test=x_test, y_test=y_test)

"""Run a linear regression with normalized data."""

reg_model = LinearReg(x_train=x_train, y_train=y_train, x_test=x_test, y_test=y_test, normalize=True)

"""Plot a histogram of the wind data before and after balancing."""

print(y_train.value_counts())

x_train, y_train, x_test, y_test = load_dataset()
y_train.hist()
plt.show()
x_train, y_train_2 = balance_train_data(x_train, y_train)
y_train_2.hist(label = "2nd Hist")
plt.show()

y_train_2.hist()
y_train_2_vals = y_train_2.value_counts()
y_train_vals = y_train.value_counts()
print(f"before and after value counts are equal is {y_train_2_vals.equals(y_train_vals)}")

"""Process data with validation."""

x_train, y_train, x_val, y_val, x_test, y_test = load_dataset(process=True, validation=True)

"""Create, train, and test a neural network."""

from tensorflow.keras import layers
from tensorflow import keras
import tensorflow as tf

def angle_subtract(a1, a2):
    """
    computes a1 - a2
    """

    d = a1 - a2
    if d < 0:
        d += 360
    return d

def angle_diff(a1, a2):
    """
    finds the smallest distance around compass between two angles
    """
    a = angle_subtract(a1, a2)
    b = angle_subtract(a2, a1)
    return min(a, b)

def tensor_angle_diff(y_true, y_pred):
  sub1 = tf.math.floormod(tf.math.subtract(y_true, y_pred), 360)
  sub2 = tf.math.floormod(tf.math.subtract(y_pred, y_true), 360)
  loss = tf.math.minimum(sub1, sub2)
  return loss

def compass_loss(y_true, y_pred, tensor_angle_diff=tensor_angle_diff):
  # min(floor_mod(subtrac(y_true, y_pred)))
  elems = (y_true, y_pred)
  # print(f"y_true.shape = {y_true.shape}, y_pred.shape = {y_pred.shape}")
  ans = tf.map_fn(lambda x: tensor_angle_diff(x[0], x[1]), elems, fn_output_signature=tf.float32)
  return tf.reduce_mean(ans)


def list_compass_loss(y_true, y_pred):
  losses = []
  for y, pred in zip(y_true, y_pred):
    loss = angle_diff(y, pred)
    losses.append(loss)
  mean_loss = sum(losses) / len(losses)
  return mean_loss
  

def build_model(activation, hypes=hypes):
  model = keras.Sequential()
  # hidden layer size = 100 more than input
  input_size = hypes.size * hypes.minutes
  hidden_size = input_size + 100
  model.add(layers.Dense(hidden_size, activation=activation))  # hidden layer 1
  model.add(layers.Dense(hidden_size, activation=activation))  # hidden layer 2
  model.add(layers.Dense(hidden_size, activation=activation))  # hidden layer 3
  model.add(layers.Dense(1, activation="linear"))
  model.add(layers.Lambda(lambda x: x % 360))  # output layer, keep in compass range
  return model

def cross_val_nn_lr(x_train, y_train, x_val, y_val, x_test, y_test, hypes=hypes, compass_loss=compass_loss):
  lr_tests = 20
  best_lr = 0
  best_mse = math.inf
  best_model = None
  epochs_test = 3

  for i in range(lr_tests):
    lr = 10**np.random.uniform(-6,-3)
    model = build_model('relu')
    model.compile(loss = compass_loss, optimizer = keras.optimizers.Adam(learning_rate = lr), metrics=[compass_loss])
    history = model.fit(x_train, y_train, batch_size=16, epochs=epochs_test, validation_data=(x_val, y_val))
    print(history)
    if (history.history['compass_loss'][-1] < best_mse):
      best_mse = history.history['compass_loss'][-1]
      best_lr = lr
      best_model = model

  print(f"\n\n\n\n LR Cross Validation OVER")
  print("Best hyperparams")
  print(f"best_mse = {best_mse}")
  print(f"best_lr = {best_lr}")
  hypes.best_lr = best_lr

  print("Testing best lr Model")
  predictions = best_model.predict(x_test)
  display_metrics(predictions, y_test)


def cross_val_nn_act(x_train, y_train, x_val, y_val, x_test, y_test, hypes=hypes, compass_loss=compass_loss):
  best_mse = math.inf
  best_activation = 'relu'
  best_model = None
  activations_to_try = ['relu', 'elu', 'tanh']
  epochs_test = 3

  for activation in activations_to_try:
    model = build_model(activation)
    model.compile(loss = compass_loss, optimizer = keras.optimizers.Adam(learning_rate = hypes.best_lr), metrics=[compass_loss])
    history = model.fit(x_train, y_train, batch_size=16, epochs=epochs_test, validation_data=(x_val, y_val))
    #Modified if statement to account for fact that we are using compass loss
    if (history.history['compass_loss'][-1] < best_mse):
      best_mse = history.history['compass_loss'][-1]
      best_activation = activation
      best_model = model

  print(f"\n\n\n\nActivation Cross Validation Over")
  print("Best hyperparams")
  print(f"best_mse = {best_mse}")
  print(f"best_activation = {best_activation}")
  hypes.best_activation = best_activation

  print("Testing best_activation Model")
  predictions = best_model.predict(x_test)
  display_metrics(predictions, y_test)

def train_best_nn(x_train, y_train, x_test, y_test, hypes=hypes, compass_loss=compass_loss):
  print(f"Best lr = {hypes.best_lr}")
  print(f"Best Activation = {hypes.best_activation}")
  epochs_test = 10
  model = build_model(hypes.best_activation)
  model.compile(loss = compass_loss, optimizer = keras.optimizers.Adam(learning_rate = hypes.best_lr), metrics=[compass_loss])
  history = model.fit(x_train, y_train, batch_size=16, epochs=epochs_test)
  print("Testing the Best Model")
  predictions = model.predict(x_test)
  print("Predictions:")
  predictions = pd.DataFrame(predictions)
  with pd.option_context('display.max_rows', None, 'display.max_columns', None):
    print(predictions)
  display_metrics(predictions, y_test)

x_train, y_train, x_val, y_val, x_test, y_test = load_dataset(validation=True)
cross_val_nn_lr(x_train, y_train, x_val, y_val, x_test, y_test)
cross_val_nn_act(x_train, y_train, x_val, y_val, x_test, y_test)

train_best_nn(x_train, y_train, x_test, y_test)

"""Reshape data for conv1d."""

# split a 2d dataframe into a 3d array of shape (samples, timesteps, features)
def data_to_3d(data, hypes=hypes):
  # N = number of equally sized arrays = number of time_steps
  og_shape = data.shape
  time_steps = hypes.size * hypes.minutes
  n_features = len(hypes.features)
  data = data.to_numpy()
  data = data.reshape((og_shape[0], time_steps, n_features))
  print(f"reshaped shape = {np.shape(data)}")
  return data

x_train, y_train, x_test, y_test = load_dataset()

x_train = data_to_3d(x_train)
x_test = data_to_3d(x_test)

"""Defining the conv1d model."""

from tensorflow.keras.layers import Conv1D, Dropout, MaxPooling1D, Flatten, Dense
# shape should be (samples, timesteps, features)
def build_conv1d(hypes=hypes):
  time_steps = hypes.size*hypes.minutes
  temporal_window = time_steps // 3
  n_features = len(hypes.features)
  model = keras.Sequential()
  model.add(Conv1D(filters=64, kernel_size=temporal_window, activation='relu', input_shape=(time_steps, n_features)))
  model.add(Conv1D(filters=64, kernel_size=temporal_window, activation='relu'))
  model.add(Dropout(0.5))
  model.add(MaxPooling1D(pool_size=2))
  model.add(Flatten())
  model.add(Dense(100, activation='relu'))
  model.add(layers.Dense(1, activation="linear"))
  model.add(layers.Lambda(lambda x: x % 360))  # output layer, keep in compass range
  return model

"""Training and testing a 1D convolutional neural network."""

def train_conv1d(x_train, y_train, x_test, y_test, hypes=hypes, compass_loss=compass_loss):
  epochs_test = 20
  model = build_conv1d()
  model.compile(loss = compass_loss, optimizer = 'adam', metrics=[compass_loss])
  history = model.fit(x_train, y_train, batch_size=16, epochs=epochs_test)
  print("Testing the Best Model")
  predictions = model.predict(x_test)
  test_loss = compass_loss(predictions, y_test.astype('float32'))
  print(f"Test compass loss = {test_loss}")
  predictions = pd.DataFrame(predictions)
  print("Predictions:")
  with pd.option_context('display.max_rows', None, 'display.max_columns', None):
    print(predictions)

# x_train = data_to_3d(x_train)
# x_test = data_to_3d(x_test)
train_conv1d(x_train, y_train, x_test, y_test)

"""Finally, let's try some categorical models to see how they do. We'll need to convert the true wind directions into integers, and encode them as one hot vectors."""

# we only need to encode y values
def encode_twds(y_data):
  y_data = y_data.round()
  y_data = y_data.astype("int32")
  print(type(y_data))
  return y_data

from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report
def train_test_logistic(x_train, y_train, x_test, y_test):
  y_train = np.ravel(y_train)
  y_test = np.ravel(y_test)
  log_reg = LogisticRegression(solver='saga', class_weight = 'balanced', random_state=123)
  log_reg.fit(x_train, y_train)
  predicitons = log_reg.predict(x_test)
  print(predictions)
  res = classification_report(y_test, predictions)
  print(res)

x_train, y_train, x_test, y_test = load_dataset(process=True)
print(f"x_train, y_train, x_test, y_test shape = {x_train.shape}, {y_train.shape}, {x_test.shape}, {y_test.shape}")

print(f"y_train shape = {y_train.shape}")
y_train = encode_twds(y_train)
print(f"y_train shape = {y_train.shape}")
y_test = encode_twds(y_test)

print(y_train)
train_test_logistic(x_train, y_train, x_test, y_test)

"""Trying a Random Forest Regressor."""

from sklearn.ensemble import RandomForestRegressor
rf = RandomForestRegressor(random_state=123)
y_train = np.ravel(y_train)
rf.fit(x_train, y_train)
predicitons = rf.predict(x_test)
rf_loss = list_compass_loss(y_test, predictions)
print(f"Random forest compass loss = {rf_loss}")